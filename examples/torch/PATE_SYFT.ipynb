{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learning private models with multiple teachers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Protocol:\n",
    "1. Train teachers:\n",
    "    - Devide training set into buckets (not overlapping)\n",
    "    - Train a models (teacher) on each bucket\n",
    "2. Train student:\n",
    "    - Extract a share of the test set\n",
    "    - Ensemble predictions from teachers: queries each teacher for predictions on the test set share\n",
    "    - Aggregate teacher predictions to get student training labels using noising max: it\n",
    "  adds Laplacian noise to label counts and returns the most frequent label\n",
    "    - Train student with the aggregated label\n",
    "    - Validate the student model on the remaining test data\n",
    "\n",
    "http://www.cleverhans.io/privacy/2018/04/29/privacy-and-machine-learning.html\n",
    "https://github.com/tensorflow/models/tree/master/research/differential_privacy/multiple_teachers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import argparse\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "\n",
    "\n",
    "def prepare_mnist():\n",
    "    kwargs = {\"num_workers\": 1}\n",
    "\n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "        datasets.MNIST(\n",
    "            \"./data\",\n",
    "            train=True,\n",
    "            download=True,\n",
    "            transform=transforms.Compose(\n",
    "                [transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,))]\n",
    "            ),\n",
    "        ),\n",
    "        batch_size=60000,\n",
    "        shuffle=True,\n",
    "        **kwargs,\n",
    "    )\n",
    "\n",
    "    test_loader = torch.utils.data.DataLoader(\n",
    "        datasets.MNIST(\n",
    "            \"./data\",\n",
    "            train=False,\n",
    "            transform=transforms.Compose(\n",
    "                [transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,))]\n",
    "            ),\n",
    "        ),\n",
    "        batch_size=10000,\n",
    "        shuffle=False,\n",
    "        **kwargs,\n",
    "    )\n",
    "\n",
    "    train_data, train_labels = next(iter(train_loader))\n",
    "    test_data, test_labels = next(iter(test_loader))\n",
    "\n",
    "    return train_data, train_labels, test_data, test_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For this demo, we use MNIST dataset\n",
    "train_data, train_labels, test_data, test_labels = prepare_mnist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3.1\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "print(torch.__version__)\n",
    "# Training settings\n",
    "parser = argparse.ArgumentParser(description='PyTorch Example')\n",
    "parser.add_argument('--batch-size', type=int, default=32, metavar='N',\n",
    "                    help='input batch size for training (default: 8)')\n",
    "parser.add_argument('--test-batch-size', type=int, default=8, metavar='N',\n",
    "                    help='input batch size for testing (default: 8)')\n",
    "parser.add_argument('--epochs', type=int, default=10, metavar='N',\n",
    "                    help='number of epochs to train (default: 10)')\n",
    "parser.add_argument('--lr', type=float, default=0.001, metavar='LR',\n",
    "                    help='learning rate (default: 0.001)')\n",
    "parser.add_argument('--momentum', type=float, default=0.0, metavar='M',\n",
    "                    help='SGD momentum (default: 0.0)')\n",
    "parser.add_argument('--seed', type=int, default=1, metavar='S',\n",
    "                    help='random seed (default: 1)')\n",
    "parser.add_argument('--log-interval', type=int, default=10, metavar='N',\n",
    "                    help='how many batches to wait before logging training status')\n",
    "args = parser.parse_args([])\n",
    "\n",
    "torch.manual_seed(args.seed)\n",
    "kwargs = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = \"mnist\" \n",
    "nb_labels = 10\n",
    "nb_teachers = 100 \n",
    "stdnt_share = 1000\n",
    "lap_scale = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import syft as sy\n",
    "from syft import Variable as Var\n",
    "from syft import nn\n",
    "from syft import optim\n",
    "\n",
    "from syft.dp.pate import partition_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "hook = sy.TorchHook(verbose=False)\n",
    "me = hook.local_worker\n",
    "\n",
    "teacher_nodes = []\n",
    "for i in range(nb_teachers):\n",
    "    teacher_nodes.append(sy.VirtualWorker(id=str(i), hook=hook))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def distribute_training_data_across_teachers(train_data, train_labels, nb_teachers, teacher_nodes):\n",
    "    train_distributed_dataset_all_teachers = {}\n",
    "\n",
    "    for i in range(len(teacher_nodes[:11])):\n",
    "        train_distributed_dataset = []\n",
    "        teacher_id = int(teacher_nodes[i].id)\n",
    "        data, labels = partition_dataset(train_data, train_labels, nb_teachers, teacher_id)\n",
    "        train = TensorDataset(data, labels)\n",
    "        train_loader = DataLoader(train, batch_size=args.batch_size, shuffle=True, **kwargs)\n",
    "        for batch_idx, (data,labels) in enumerate(train_loader):\n",
    "            data = Variable(data)\n",
    "            labels = Variable(labels.type(torch.LongTensor))\n",
    "            data.send(teacher_nodes[teacher_id])\n",
    "            labels.send(teacher_nodes[teacher_id])\n",
    "            train_distributed_dataset.append((data, labels))\n",
    "\n",
    "        train_distributed_dataset_all_teachers[teacher_id]= train_distributed_dataset\n",
    "        \n",
    "    return train_distributed_dataset_all_teachers\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_distributed_dataset_all_teachers = distribute_training_data_across_teachers(train_data, train_labels, nb_teachers, teacher_nodes)\n",
    "\n",
    "test = TensorDataset(test_data, test_labels)\n",
    "test_loader = DataLoader(test, batch_size=args.batch_size, shuffle=True, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN_Model(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(CNN_Model, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1,16,5,stride=1)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.avgpool1 = nn.AvgPool2d(2)\n",
    "        self.linear1 = nn.Linear(2304, 100)\n",
    "        self.linear2 = nn.Linear(100, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.relu1(x)\n",
    "        x = self.avgpool1(x)\n",
    "        x = x.view(-1, 2304)\n",
    "        x = self.linear1(x)\n",
    "        out = self.linear2(x)\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_teachers(\n",
    "    model,\n",
    "    train_data,\n",
    "    train_labels,\n",
    "    test_data,\n",
    "    test_labels,\n",
    "    nb_teachers,\n",
    "    teacher_id,\n",
    "    filename,\n",
    "):\n",
    "    data, labels = partition_dataset(train_data, train_labels, nb_teachers, teacher_id)\n",
    "\n",
    "    train_prep = PrepareData(data, labels)\n",
    "    train_loader = DataLoader(train_prep, batch_size=64, shuffle=True)\n",
    "\n",
    "    test_prep = PrepareData(test_data, test_labels)\n",
    "    test_loader = DataLoader(test_prep, batch_size=64, shuffle=False)\n",
    "\n",
    "    print(\"\\nTrain teacher ID: \" + str(teacher_id))\n",
    "\n",
    "    train(model, train_loader, test_loader, ckpt_path, filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_teacher(model, train_distributed_dataset, test_loader, ckpt_path, filename):\n",
    "    \n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "    \n",
    "    for epoch in range(args.epochs):\n",
    "        model.train()\n",
    "\n",
    "        train_num = 0\n",
    "        correct = 0\n",
    "        for batch_idx, (data,target) in enumerate(train_distributed_dataset):\n",
    "\n",
    "            worker = data.location\n",
    "            model.send(worker)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            # update the model\n",
    "            output = model(data)\n",
    "            loss = F.cross_entropy(output, target, size_average=False)\n",
    "            loss.backward()\n",
    "            model.get()\n",
    "            optimizer.step()\n",
    "\n",
    "            pred_label = output.max(1, keepdim=True)[1]  # get the index of the max logit\n",
    "            pred_label.get()\n",
    "            target.get()\n",
    "\n",
    "            train_num += len(target)\n",
    "            correct += int(\n",
    "                    pred_label.eq(target.view_as(pred_label)).sum()\n",
    "                 )  # add to running total of hits\n",
    "            target.send(worker)\n",
    "\n",
    "\n",
    "        print(\"Train Accuracy: {}/{} ({:.0f}%)\".format(\n",
    "                correct, int(train_num), 100.0 * float(correct / train_num)))\n",
    "\n",
    "    \n",
    "    # set up training metrics we want to track\n",
    "    test_correct = 0\n",
    "    test_num = len(test_loader.sampler)\n",
    "\n",
    "    for ix, (img, label) in enumerate(test_loader):  # iterate over training batches\n",
    "        img = Var(img.float())\n",
    "        label = Var(label.type(torch.LongTensor))\n",
    "        optimizer.zero_grad()  # clear parameter gradients from previous training update\n",
    "        output = model(img)  # forward pass\n",
    "        # output = output.type(torch.float32)\n",
    "        loss = F.cross_entropy(\n",
    "            output, label, size_average=False\n",
    "        )  # calculate network loss\n",
    "\n",
    "        pred = output.max(1, keepdim=True)[1]  # get the index of the max logit\n",
    "        test_correct += int(\n",
    "            pred.eq(label.view_as(pred)).sum()\n",
    "        )  # add to running total of hits\n",
    "\n",
    "        # print whole epoch's training accuracy; useful for monitoring overfitting\n",
    "    print(\n",
    "        \"Test Accuracy: {}/{} ({:.0f}%)\".format(\n",
    "            test_correct, test_num, 100.0 * test_correct / test_num\n",
    "        )\n",
    "    )\n",
    "\n",
    "    if not os.path.isdir(ckpt_path):\n",
    "        os.makedirs(ckpt_path)\n",
    "        \n",
    "    #torch.save(model.state_dict(), ckpt_path + filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Train teacher ID: 0\n",
      "Train Accuracy: 333/600 (56%)\n",
      "Train Accuracy: 517/600 (86%)\n",
      "Test Accuracy: 8544/10000 (85%)\n",
      "\n",
      "Train teacher ID: 1\n",
      "Train Accuracy: 283/600 (47%)\n",
      "Train Accuracy: 519/600 (86%)\n",
      "Test Accuracy: 8686/10000 (87%)\n",
      "\n",
      "Train teacher ID: 2\n",
      "Train Accuracy: 316/600 (53%)\n",
      "Train Accuracy: 503/600 (84%)\n",
      "Test Accuracy: 8688/10000 (87%)\n",
      "\n",
      "Train teacher ID: 3\n",
      "Train Accuracy: 287/600 (48%)\n",
      "Train Accuracy: 505/600 (84%)\n",
      "Test Accuracy: 8181/10000 (82%)\n",
      "\n",
      "Train teacher ID: 4\n",
      "Train Accuracy: 338/600 (56%)\n",
      "Train Accuracy: 532/600 (89%)\n",
      "Test Accuracy: 8694/10000 (87%)\n",
      "\n",
      "Train teacher ID: 5\n",
      "Train Accuracy: 339/600 (56%)\n",
      "Train Accuracy: 538/600 (90%)\n",
      "Test Accuracy: 8833/10000 (88%)\n",
      "\n",
      "Train teacher ID: 6\n",
      "Train Accuracy: 324/600 (54%)\n",
      "Train Accuracy: 510/600 (85%)\n",
      "Test Accuracy: 8672/10000 (87%)\n",
      "\n",
      "Train teacher ID: 7\n",
      "Train Accuracy: 290/600 (48%)\n",
      "Train Accuracy: 514/600 (86%)\n",
      "Test Accuracy: 8450/10000 (84%)\n",
      "\n",
      "Train teacher ID: 8\n",
      "Train Accuracy: 333/600 (56%)\n",
      "Train Accuracy: 531/600 (88%)\n",
      "Test Accuracy: 8839/10000 (88%)\n",
      "\n",
      "Train teacher ID: 9\n",
      "Train Accuracy: 302/600 (50%)\n",
      "Train Accuracy: 507/600 (84%)\n",
      "Test Accuracy: 8716/10000 (87%)\n"
     ]
    }
   ],
   "source": [
    "args.epochs = 2\n",
    "nb_teachers = 10\n",
    "#for i in range(nb_teachers):\n",
    "    \n",
    "for i in range(len(teacher_nodes[:nb_teachers])):\n",
    "    teacher_id = int(teacher_nodes[i].id)\n",
    "    \n",
    "    ckpt_path = 'checkpoint/'\n",
    "    \n",
    "    filename = str(dataset) + '_' + str(nb_teachers) + '_teachers_' + str(teacher_id) + '.pth'\n",
    "\n",
    "    train_distributed_dataset = train_distributed_dataset_all_teachers[teacher_id]\n",
    "    \n",
    "    model = CNN_Model(10)\n",
    "    \n",
    "    print(\"\\nTrain teacher ID: \" + str(teacher_id))\n",
    "    \n",
    "    train_teacher(model, train_distributed_dataset, test_loader, ckpt_path, filename)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ckpt_path = 'checkpoint/'\n",
    "    \n",
    "# filename = str(\"mnist\") + '_' + str(10) + '_teachers_' + str(1) + '.pth'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = CNN_Model(10)\n",
    "# torch.save(model.state_dict(), ckpt_path + filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(teacher_nodes[:nb_teachers])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
