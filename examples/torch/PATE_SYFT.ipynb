{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learning private models with multiple teachers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Protocol:\n",
    "1. Train teachers:\n",
    "    - Devide training set into buckets (not overlapping)\n",
    "    - Train a models (teacher) on each bucket\n",
    "2. Train student:\n",
    "    - Extract a share of the test set\n",
    "    - Ensemble predictions from teachers: queries each teacher for predictions on the test set share\n",
    "    - Aggregate teacher predictions to get student training labels using noising max: it\n",
    "  adds Laplacian noise to label counts and returns the most frequent label\n",
    "    - Train student with the aggregated label\n",
    "    - Validate the student model on the remaining test data\n",
    "\n",
    "http://www.cleverhans.io/privacy/2018/04/29/privacy-and-machine-learning.html\n",
    "https://github.com/tensorflow/models/tree/master/research/differential_privacy/multiple_teachers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from syft.dp.pate import train_teachers, train_student\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def prepare_mnist():\n",
    "    kwargs = {\"num_workers\": 1}\n",
    "\n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "        datasets.MNIST(\n",
    "            \"./data\",\n",
    "            train=True,\n",
    "            download=True,\n",
    "            transform=transforms.Compose(\n",
    "                [transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,))]\n",
    "            ),\n",
    "        ),\n",
    "        batch_size=60000,\n",
    "        shuffle=True,\n",
    "        **kwargs,\n",
    "    )\n",
    "\n",
    "    test_loader = torch.utils.data.DataLoader(\n",
    "        datasets.MNIST(\n",
    "            \"./data\",\n",
    "            train=False,\n",
    "            transform=transforms.Compose(\n",
    "                [transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,))]\n",
    "            ),\n",
    "        ),\n",
    "        batch_size=10000,\n",
    "        shuffle=False,\n",
    "        **kwargs,\n",
    "    )\n",
    "\n",
    "    train_data, train_labels = next(iter(train_loader))\n",
    "    test_data, test_labels = next(iter(test_loader))\n",
    "\n",
    "    return train_data, train_labels, test_data, test_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For this demo, we use MNIST dataset\n",
    "train_data, train_labels, test_data, test_labels = prepare_mnist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3.1\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "print(torch.__version__)\n",
    "# Training settings\n",
    "parser = argparse.ArgumentParser(description='PyTorch Example')\n",
    "parser.add_argument('--batch-size', type=int, default=32, metavar='N',\n",
    "                    help='input batch size for training (default: 8)')\n",
    "parser.add_argument('--test-batch-size', type=int, default=8, metavar='N',\n",
    "                    help='input batch size for testing (default: 8)')\n",
    "parser.add_argument('--epochs', type=int, default=10, metavar='N',\n",
    "                    help='number of epochs to train (default: 10)')\n",
    "parser.add_argument('--lr', type=float, default=0.001, metavar='LR',\n",
    "                    help='learning rate (default: 0.001)')\n",
    "parser.add_argument('--momentum', type=float, default=0.0, metavar='M',\n",
    "                    help='SGD momentum (default: 0.0)')\n",
    "parser.add_argument('--seed', type=int, default=1, metavar='S',\n",
    "                    help='random seed (default: 1)')\n",
    "parser.add_argument('--log-interval', type=int, default=10, metavar='N',\n",
    "                    help='how many batches to wait before logging training status')\n",
    "args = parser.parse_args([])\n",
    "\n",
    "torch.manual_seed(args.seed)\n",
    "kwargs = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = \"mnist\" \n",
    "nb_labels = 10\n",
    "nb_teachers = 100 \n",
    "stdnt_share = 1000\n",
    "lap_scale = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import syft as sy\n",
    "from syft import Variable as Var\n",
    "from syft import nn\n",
    "from syft import optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hook = sy.TorchHook(verbose=False)\n",
    "# me = hook.local_worker\n",
    "# bob = sy.VirtualWorker(id=\"bob\",hook=hook, is_client_worker=False)\n",
    "# alice = sy.VirtualWorker(id=\"alice\",hook=hook, is_client_worker=False)\n",
    "# me.is_client_worker = False\n",
    "\n",
    "# compute_nodes = [bob, alice]\n",
    "\n",
    "# bob.add_workers([alice])\n",
    "# alice.add_workers([bob])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Worker 0 already exists. Replacing old worker which could cause unexpected behavior\n",
      "WARNING:root:Worker 1 already exists. Replacing old worker which could cause unexpected behavior\n",
      "WARNING:root:Worker 2 already exists. Replacing old worker which could cause unexpected behavior\n",
      "WARNING:root:Worker 3 already exists. Replacing old worker which could cause unexpected behavior\n",
      "WARNING:root:Worker 4 already exists. Replacing old worker which could cause unexpected behavior\n",
      "WARNING:root:Worker 5 already exists. Replacing old worker which could cause unexpected behavior\n",
      "WARNING:root:Worker 6 already exists. Replacing old worker which could cause unexpected behavior\n",
      "WARNING:root:Worker 7 already exists. Replacing old worker which could cause unexpected behavior\n",
      "WARNING:root:Worker 8 already exists. Replacing old worker which could cause unexpected behavior\n",
      "WARNING:root:Worker 9 already exists. Replacing old worker which could cause unexpected behavior\n",
      "WARNING:root:Worker 10 already exists. Replacing old worker which could cause unexpected behavior\n",
      "WARNING:root:Worker 11 already exists. Replacing old worker which could cause unexpected behavior\n",
      "WARNING:root:Worker 12 already exists. Replacing old worker which could cause unexpected behavior\n",
      "WARNING:root:Worker 13 already exists. Replacing old worker which could cause unexpected behavior\n",
      "WARNING:root:Worker 14 already exists. Replacing old worker which could cause unexpected behavior\n",
      "WARNING:root:Worker 15 already exists. Replacing old worker which could cause unexpected behavior\n",
      "WARNING:root:Worker 16 already exists. Replacing old worker which could cause unexpected behavior\n",
      "WARNING:root:Worker 17 already exists. Replacing old worker which could cause unexpected behavior\n",
      "WARNING:root:Worker 18 already exists. Replacing old worker which could cause unexpected behavior\n",
      "WARNING:root:Worker 19 already exists. Replacing old worker which could cause unexpected behavior\n",
      "WARNING:root:Worker 20 already exists. Replacing old worker which could cause unexpected behavior\n",
      "WARNING:root:Worker 21 already exists. Replacing old worker which could cause unexpected behavior\n",
      "WARNING:root:Worker 22 already exists. Replacing old worker which could cause unexpected behavior\n",
      "WARNING:root:Worker 23 already exists. Replacing old worker which could cause unexpected behavior\n",
      "WARNING:root:Worker 24 already exists. Replacing old worker which could cause unexpected behavior\n",
      "WARNING:root:Worker 25 already exists. Replacing old worker which could cause unexpected behavior\n",
      "WARNING:root:Worker 26 already exists. Replacing old worker which could cause unexpected behavior\n",
      "WARNING:root:Worker 27 already exists. Replacing old worker which could cause unexpected behavior\n",
      "WARNING:root:Worker 28 already exists. Replacing old worker which could cause unexpected behavior\n",
      "WARNING:root:Worker 29 already exists. Replacing old worker which could cause unexpected behavior\n",
      "WARNING:root:Worker 30 already exists. Replacing old worker which could cause unexpected behavior\n",
      "WARNING:root:Worker 31 already exists. Replacing old worker which could cause unexpected behavior\n",
      "WARNING:root:Worker 32 already exists. Replacing old worker which could cause unexpected behavior\n",
      "WARNING:root:Worker 33 already exists. Replacing old worker which could cause unexpected behavior\n",
      "WARNING:root:Worker 34 already exists. Replacing old worker which could cause unexpected behavior\n",
      "WARNING:root:Worker 35 already exists. Replacing old worker which could cause unexpected behavior\n",
      "WARNING:root:Worker 36 already exists. Replacing old worker which could cause unexpected behavior\n",
      "WARNING:root:Worker 37 already exists. Replacing old worker which could cause unexpected behavior\n",
      "WARNING:root:Worker 38 already exists. Replacing old worker which could cause unexpected behavior\n",
      "WARNING:root:Worker 39 already exists. Replacing old worker which could cause unexpected behavior\n",
      "WARNING:root:Worker 40 already exists. Replacing old worker which could cause unexpected behavior\n",
      "WARNING:root:Worker 41 already exists. Replacing old worker which could cause unexpected behavior\n",
      "WARNING:root:Worker 42 already exists. Replacing old worker which could cause unexpected behavior\n",
      "WARNING:root:Worker 43 already exists. Replacing old worker which could cause unexpected behavior\n",
      "WARNING:root:Worker 44 already exists. Replacing old worker which could cause unexpected behavior\n",
      "WARNING:root:Worker 45 already exists. Replacing old worker which could cause unexpected behavior\n",
      "WARNING:root:Worker 46 already exists. Replacing old worker which could cause unexpected behavior\n",
      "WARNING:root:Worker 47 already exists. Replacing old worker which could cause unexpected behavior\n",
      "WARNING:root:Worker 48 already exists. Replacing old worker which could cause unexpected behavior\n",
      "WARNING:root:Worker 49 already exists. Replacing old worker which could cause unexpected behavior\n",
      "WARNING:root:Worker 50 already exists. Replacing old worker which could cause unexpected behavior\n",
      "WARNING:root:Worker 51 already exists. Replacing old worker which could cause unexpected behavior\n",
      "WARNING:root:Worker 52 already exists. Replacing old worker which could cause unexpected behavior\n",
      "WARNING:root:Worker 53 already exists. Replacing old worker which could cause unexpected behavior\n",
      "WARNING:root:Worker 54 already exists. Replacing old worker which could cause unexpected behavior\n",
      "WARNING:root:Worker 55 already exists. Replacing old worker which could cause unexpected behavior\n",
      "WARNING:root:Worker 56 already exists. Replacing old worker which could cause unexpected behavior\n",
      "WARNING:root:Worker 57 already exists. Replacing old worker which could cause unexpected behavior\n",
      "WARNING:root:Worker 58 already exists. Replacing old worker which could cause unexpected behavior\n",
      "WARNING:root:Worker 59 already exists. Replacing old worker which could cause unexpected behavior\n",
      "WARNING:root:Worker 60 already exists. Replacing old worker which could cause unexpected behavior\n",
      "WARNING:root:Worker 61 already exists. Replacing old worker which could cause unexpected behavior\n",
      "WARNING:root:Worker 62 already exists. Replacing old worker which could cause unexpected behavior\n",
      "WARNING:root:Worker 63 already exists. Replacing old worker which could cause unexpected behavior\n",
      "WARNING:root:Worker 64 already exists. Replacing old worker which could cause unexpected behavior\n",
      "WARNING:root:Worker 65 already exists. Replacing old worker which could cause unexpected behavior\n",
      "WARNING:root:Worker 66 already exists. Replacing old worker which could cause unexpected behavior\n",
      "WARNING:root:Worker 67 already exists. Replacing old worker which could cause unexpected behavior\n",
      "WARNING:root:Worker 68 already exists. Replacing old worker which could cause unexpected behavior\n",
      "WARNING:root:Worker 69 already exists. Replacing old worker which could cause unexpected behavior\n",
      "WARNING:root:Worker 70 already exists. Replacing old worker which could cause unexpected behavior\n",
      "WARNING:root:Worker 71 already exists. Replacing old worker which could cause unexpected behavior\n",
      "WARNING:root:Worker 72 already exists. Replacing old worker which could cause unexpected behavior\n",
      "WARNING:root:Worker 73 already exists. Replacing old worker which could cause unexpected behavior\n",
      "WARNING:root:Worker 74 already exists. Replacing old worker which could cause unexpected behavior\n",
      "WARNING:root:Worker 75 already exists. Replacing old worker which could cause unexpected behavior\n",
      "WARNING:root:Worker 76 already exists. Replacing old worker which could cause unexpected behavior\n",
      "WARNING:root:Worker 77 already exists. Replacing old worker which could cause unexpected behavior\n",
      "WARNING:root:Worker 78 already exists. Replacing old worker which could cause unexpected behavior\n",
      "WARNING:root:Worker 79 already exists. Replacing old worker which could cause unexpected behavior\n",
      "WARNING:root:Worker 80 already exists. Replacing old worker which could cause unexpected behavior\n",
      "WARNING:root:Worker 81 already exists. Replacing old worker which could cause unexpected behavior\n",
      "WARNING:root:Worker 82 already exists. Replacing old worker which could cause unexpected behavior\n",
      "WARNING:root:Worker 83 already exists. Replacing old worker which could cause unexpected behavior\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Worker 84 already exists. Replacing old worker which could cause unexpected behavior\n",
      "WARNING:root:Worker 85 already exists. Replacing old worker which could cause unexpected behavior\n",
      "WARNING:root:Worker 86 already exists. Replacing old worker which could cause unexpected behavior\n",
      "WARNING:root:Worker 87 already exists. Replacing old worker which could cause unexpected behavior\n",
      "WARNING:root:Worker 88 already exists. Replacing old worker which could cause unexpected behavior\n",
      "WARNING:root:Worker 89 already exists. Replacing old worker which could cause unexpected behavior\n",
      "WARNING:root:Worker 90 already exists. Replacing old worker which could cause unexpected behavior\n",
      "WARNING:root:Worker 91 already exists. Replacing old worker which could cause unexpected behavior\n",
      "WARNING:root:Worker 92 already exists. Replacing old worker which could cause unexpected behavior\n",
      "WARNING:root:Worker 93 already exists. Replacing old worker which could cause unexpected behavior\n",
      "WARNING:root:Worker 94 already exists. Replacing old worker which could cause unexpected behavior\n",
      "WARNING:root:Worker 95 already exists. Replacing old worker which could cause unexpected behavior\n",
      "WARNING:root:Worker 96 already exists. Replacing old worker which could cause unexpected behavior\n",
      "WARNING:root:Worker 97 already exists. Replacing old worker which could cause unexpected behavior\n",
      "WARNING:root:Worker 98 already exists. Replacing old worker which could cause unexpected behavior\n",
      "WARNING:root:Worker 99 already exists. Replacing old worker which could cause unexpected behavior\n"
     ]
    }
   ],
   "source": [
    "hook = sy.TorchHook(verbose=False)\n",
    "me = hook.local_worker\n",
    "\n",
    "compute_nodes = []\n",
    "for i in range(nb_teachers):\n",
    "    compute_nodes.append(sy.VirtualWorker(id=str(i), hook=hook))\n",
    "    \n",
    "for i in range(len(compute_nodes)):\n",
    "#    compute_nodes[i].add_workers([compute_nodes[i+1]])\n",
    "    me.add_worker(compute_nodes[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#compute_nodes[0].add_workers(compute_nodes[1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from syft.dp.pate import partition_dataset\n",
    "\n",
    "# train_distributed_dataset = []\n",
    "\n",
    "# for i in range(len(compute_nodes)):\n",
    "#     worker_id = int(compute_nodes[i].id)\n",
    "#     data, labels = partition_dataset(train_data, train_labels, nb_teachers, worker_id)\n",
    "#     data = Variable(data)\n",
    "#     labels = Variable(labels.type(torch.LongTensor))\n",
    "#     #print(len(labels))\n",
    "#     data.send(compute_nodes[worker_id])\n",
    "#     labels.send(compute_nodes[worker_id])\n",
    "#     train_distributed_dataset.append((data, labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = TensorDataset(test_data, test_labels)\n",
    "test_loader = DataLoader(test, batch_size=args.batch_size, shuffle=True, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "24\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "24\n"
     ]
    }
   ],
   "source": [
    "from syft.dp.pate import partition_dataset\n",
    "\n",
    "train_distributed_dataset = []\n",
    "train_distributed_dataset_all_teachers = []\n",
    "\n",
    "for i in range(len(compute_nodes[:2])):\n",
    "#for i in range(1):\n",
    "    teacher_id = int(compute_nodes[i].id)\n",
    "    data, labels = partition_dataset(train_data, train_labels, nb_teachers, teacher_id)\n",
    "    train = TensorDataset(data, labels)\n",
    "    train_loader = DataLoader(train, batch_size=args.batch_size, shuffle=True, **kwargs)\n",
    "    for batch_idx, (data,labels) in enumerate(train_loader):\n",
    "        print(len(labels))\n",
    "        data = Variable(data)\n",
    "        labels = Variable(labels.type(torch.LongTensor))\n",
    "        data.send(compute_nodes[teacher_id])\n",
    "        labels.send(compute_nodes[teacher_id])\n",
    "        train_distributed_dataset.append((data, labels))\n",
    "        \n",
    "    train_distributed_dataset_all_teachers.append(train_distributed_dataset)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Variable containing:FloatTensor[_PointerTensor - id:21006087818 owner:me loc:0 id@loc:55719567081],\n",
       " Variable containing:LongTensor[_PointerTensor - id:15444007465 owner:me loc:0 id@loc:41056433571])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_distributed_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN_Model(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(CNN_Model, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1,16,5,stride=1)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.avgpool1 = nn.AvgPool2d(2)\n",
    "        self.linear1 = nn.Linear(2304, 100)\n",
    "        self.linear2 = nn.Linear(100, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.relu1(x)\n",
    "        x = self.avgpool1(x)\n",
    "        x = x.view(-1, 2304)\n",
    "        x = self.linear1(x)\n",
    "        out = self.linear2(x)\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#yo = model(Variable(train_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#yo.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epoch, train_distributed_dataset):\n",
    "    model.train()\n",
    "    \n",
    "    train_num = 0\n",
    "    correct = 0\n",
    "    for batch_idx, (data,target) in enumerate(train_distributed_dataset):\n",
    "            \n",
    "        worker = data.location\n",
    "        model.send(worker)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        # update the model\n",
    "        output = model(data)\n",
    "        loss = F.cross_entropy(output, target, size_average=False)\n",
    "        loss.backward()\n",
    "        model.get()\n",
    "        optimizer.step()\n",
    "        \n",
    "        pred_label = output.max(1, keepdim=True)[1]  # get the index of the max logit\n",
    "        pred_label.get()\n",
    "        target.get()\n",
    "        \n",
    "        train_num += len(target)\n",
    "        correct += int(\n",
    "                pred_label.eq(target.view_as(pred_label)).sum()\n",
    "             )  # add to running total of hits\n",
    "        target.send(worker)\n",
    "        \n",
    "\n",
    "    print(\"Train Accuracy: {}/{} ({:.0f}%)\".format(\n",
    "            correct, int(train_num), 100.0 * float(correct / train_num)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy: 757/1200 (63%)\n",
      "Train Accuracy: 996/1200 (83%)\n",
      "Train Accuracy: 1051/1200 (88%)\n",
      "Train Accuracy: 1081/1200 (90%)\n",
      "Train Accuracy: 1093/1200 (91%)\n",
      "Train Accuracy: 1104/1200 (92%)\n",
      "Train Accuracy: 1115/1200 (93%)\n",
      "Train Accuracy: 1121/1200 (93%)\n",
      "Train Accuracy: 1131/1200 (94%)\n",
      "Train Accuracy: 1144/1200 (95%)\n",
      "Test Accuracy: 8950/10000 (90%)\n",
      "Train Accuracy: 725/1200 (60%)\n",
      "Train Accuracy: 993/1200 (83%)\n",
      "Train Accuracy: 1044/1200 (87%)\n",
      "Train Accuracy: 1074/1200 (90%)\n",
      "Train Accuracy: 1088/1200 (91%)\n",
      "Train Accuracy: 1102/1200 (92%)\n",
      "Train Accuracy: 1111/1200 (93%)\n",
      "Train Accuracy: 1120/1200 (93%)\n",
      "Train Accuracy: 1130/1200 (94%)\n",
      "Train Accuracy: 1138/1200 (95%)\n",
      "Test Accuracy: 8941/10000 (89%)\n"
     ]
    }
   ],
   "source": [
    "args.epochs = 10\n",
    "nb_teachers = 2\n",
    "for i in range(nb_teachers):\n",
    "\n",
    "    train_distributed_dataset = train_distributed_dataset_all_teachers[i]\n",
    "    \n",
    "    model = CNN_Model(10)\n",
    "    optimizer = optim.SGD(model.parameters(), lr=args.lr)\n",
    "\n",
    "    for epoch in range(1, args.epochs + 1):\n",
    "        train(epoch, train_distributed_dataset)\n",
    "        \n",
    "    \n",
    "    test_correct = 0\n",
    "    test_num = len(test_loader.sampler)\n",
    "\n",
    "    for ix, (img, label) in enumerate(test_loader):  # iterate over training batches\n",
    "            # img, label = img.to(device), label.to(device) # get data, send to gpu if needed\n",
    "            img = Var(img.float())\n",
    "            # label = label.type(torch.float32)\n",
    "            label = Var(label.type(torch.LongTensor))\n",
    "            optimizer.zero_grad()  # clear parameter gradients from previous training update\n",
    "            output = model(img)  # forward pass\n",
    "            # output = output.type(torch.float32)\n",
    "            loss = F.cross_entropy(\n",
    "                output, label, size_average=False\n",
    "            )  # calculate network loss\n",
    "\n",
    "            pred = output.max(1, keepdim=True)[1]  # get the index of the max logit\n",
    "            test_correct += int(\n",
    "                pred.eq(label.view_as(pred)).sum()\n",
    "            )  # add to running total of hits\n",
    "\n",
    "            # print whole epoch's training accuracy; useful for monitoring overfitting\n",
    "    print(\"Test Accuracy: {}/{} ({:.0f}%)\".format(\n",
    "                test_correct, test_num, 100.0 * test_correct / test_num\n",
    "            )\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 8941/10000 (89%)\n"
     ]
    }
   ],
   "source": [
    "test_correct = 0\n",
    "test_num = len(test_loader.sampler)\n",
    "\n",
    "for ix, (img, label) in enumerate(test_loader):  # iterate over training batches\n",
    "        # img, label = img.to(device), label.to(device) # get data, send to gpu if needed\n",
    "        img = Var(img.float())\n",
    "        # label = label.type(torch.float32)\n",
    "        label = Var(label.type(torch.LongTensor))\n",
    "        optimizer.zero_grad()  # clear parameter gradients from previous training update\n",
    "        output = model(img)  # forward pass\n",
    "        # output = output.type(torch.float32)\n",
    "        loss = F.cross_entropy(\n",
    "            output, label, size_average=False\n",
    "        )  # calculate network loss\n",
    "\n",
    "        pred = output.max(1, keepdim=True)[1]  # get the index of the max logit\n",
    "        test_correct += int(\n",
    "            pred.eq(label.view_as(pred)).sum()\n",
    "        )  # add to running total of hits\n",
    "\n",
    "        # print whole epoch's training accuracy; useful for monitoring overfitting\n",
    "print(\"Test Accuracy: {}/{} ({:.0f}%)\".format(\n",
    "            test_correct, test_num, 100.0 * test_correct / test_num\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_distributed_dataset_all_teachers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
